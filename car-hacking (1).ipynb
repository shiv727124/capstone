{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":7049653,"sourceType":"datasetVersion","datasetId":4056951}],"dockerImageVersionId":30747,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport math\nimport random\nimport matplotlib.pyplot as plt\nimport shutil\nfrom sklearn.preprocessing import QuantileTransformer\nfrom PIL import Image\nimport warnings\nwarnings.filterwarnings(\"ignore\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-24T13:44:14.764555Z","iopub.execute_input":"2024-08-24T13:44:14.765163Z","iopub.status.idle":"2024-08-24T13:44:15.846484Z","shell.execute_reply.started":"2024-08-24T13:44:14.765133Z","shell.execute_reply":"2024-08-24T13:44:15.845554Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport pandas as pd\n\n# List of CSV files for training\ncsv_files_driving = [\"/kaggle/input/car-hacking-challenge-dataset/0_Preliminary/0_Training/Pre_train_D_0.csv\", \"/kaggle/input/car-hacking-challenge-dataset/0_Preliminary/0_Training/Pre_train_D_1.csv\", \"/kaggle/input/car-hacking-challenge-dataset/0_Preliminary/0_Training/Pre_train_D_2.csv\"]\ncsv_files_stationary = [\"/kaggle/input/car-hacking-challenge-dataset/0_Preliminary/0_Training/Pre_train_S_0.csv\", \"/kaggle/input/car-hacking-challenge-dataset/0_Preliminary/0_Training/Pre_train_S_1.csv\", \"/kaggle/input/car-hacking-challenge-dataset/0_Preliminary/0_Training/Pre_train_S_2.csv\"]\n\n# Combine all CSV files into a single DataFrame\ncombined_df = pd.DataFrame()\n\nfor file in csv_files_driving + csv_files_stationary:\n    df = pd.read_csv(file)\n    combined_df = pd.concat([combined_df, df], ignore_index=True)\n\n# Save the combined DataFrame to a new CSV file\ncombined_df.to_csv(\"/kaggle/working/Training_Final.csv\", index=False)\n\nprint(\"Files combined successfully into training.csv\")\n","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:44:15.848036Z","iopub.execute_input":"2024-08-24T13:44:15.848426Z","iopub.status.idle":"2024-08-24T13:44:40.481438Z","shell.execute_reply.started":"2024-08-24T13:44:15.848401Z","shell.execute_reply":"2024-08-24T13:44:40.480563Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Read dataset\ndf=pd.read_csv('/kaggle/working/Training_Final.csv')","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:44:40.483205Z","iopub.execute_input":"2024-08-24T13:44:40.483497Z","iopub.status.idle":"2024-08-24T13:44:44.533121Z","shell.execute_reply.started":"2024-08-24T13:44:40.483472Z","shell.execute_reply":"2024-08-24T13:44:44.532268Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.Class.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:44:44.534123Z","iopub.execute_input":"2024-08-24T13:44:44.534386Z","iopub.status.idle":"2024-08-24T13:44:45.11132Z","shell.execute_reply.started":"2024-08-24T13:44:44.534364Z","shell.execute_reply":"2024-08-24T13:44:45.110454Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.SubClass.value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:44:45.113649Z","iopub.execute_input":"2024-08-24T13:44:45.113949Z","iopub.status.idle":"2024-08-24T13:44:45.637412Z","shell.execute_reply.started":"2024-08-24T13:44:45.113926Z","shell.execute_reply":"2024-08-24T13:44:45.636536Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Transformation\nConvert tabular data to images Procedures:\n\nUse quantile transform to transform the original data samples into the scale of [0,255], representing pixel value","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import QuantileTransformer\nimport matplotlib.pyplot as plt\nimport os\n\ndef hex_to_int(hex_str):\n    \"\"\"Convert a hexadecimal string to an integer.\"\"\"\n    if isinstance(hex_str, str):\n        return int(hex_str, 16)\n    return hex_str  # If it's not a string, return the original value\n\ndef preprocess_data(df):\n    \"\"\"Convert hexadecimal fields to integers and normalize the data.\"\"\"\n    # Convert Arbitration_ID to integer\n    df['Arbitration_ID'] = df['Arbitration_ID'].apply(hex_to_int)\n\n    # Convert Data field to integer and split into separate columns\n    data_columns = df['Data'].str.split(' ', expand=True)\n\n    #print(type(data_columns))\n\n    for i in range(8):\n        data_columns[i] = data_columns[i].apply(hex_to_int)\n        data_columns.rename(columns={i: f'Data[{i}]'}, inplace=True)\n\n\n    #print(data_columns)\n\n    df = df.drop(columns=['Data'])\n    df = pd.concat([df, data_columns], axis=1)\n\n    return df\n\n# Load the combined CSV file\ndf = pd.read_csv('/kaggle/working/Training_Final.csv')\n\n# Preprocess the data\ndf_changed = preprocess_data(df)\n\n# Sample 20% of the dataset\ndf_changed = df_changed.sample(frac=0.2, random_state=42)\n\n# Reset index to keep it clean\ndf_changed = df_changed.reset_index(drop=True)\n\n# Display the resulting DataFrame\ndf_changed\n\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:44:45.638758Z","iopub.execute_input":"2024-08-24T13:44:45.639586Z","iopub.status.idle":"2024-08-24T13:45:31.583725Z","shell.execute_reply.started":"2024-08-24T13:44:45.639549Z","shell.execute_reply":"2024-08-24T13:45:31.582709Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report, accuracy_score\n\n# Load the dataset\ndata = df_changed\n\n# Handle missing values (fill NaNs with a method or drop rows)\ndata.fillna(method='ffill', inplace=True)  # Example: forward fill\n\n# Convert 'Class' to binary values (0 for Normal, 1 for Other)\ndata['Class'] = data['Class'].apply(lambda x: 0 if x == 'Normal' else 1)\n\n# Encode 'Subclass' for abnormal data (it will be NaN for normal data)\nlabel_encoder = LabelEncoder()\ndata['SubClass'] = data['SubClass'].fillna('None')  # Fill NaNs with 'None'\ndata['SubClass'] = label_encoder.fit_transform(data['SubClass'])\n\n# Define features and target variable\nfeatures = \n\n\nX = data[features]\ny = data['Class']\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Initialize the model (Random Forest in this case)\nmodel = RandomForestClassifier(n_estimators=100, random_state=42)\n\n# Train the model\nmodel.fit(X_train, y_train)\n\n# Make predictions\ny_pred = model.predict(X_test)\n\n# Evaluate the model\nprint(f\"Accuracy: {accuracy_score(y_test, y_pred)}\")\nprint(classification_report(y_test, y_pred))\n\n# Save the model if needed\nimport joblib\njoblib.dump(model, 'model.pkl')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-24T08:08:52.425101Z","iopub.execute_input":"2024-08-24T08:08:52.425358Z","iopub.status.idle":"2024-08-24T08:15:26.506707Z","shell.execute_reply.started":"2024-08-24T08:08:52.425335Z","shell.execute_reply":"2024-08-24T08:15:26.505878Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom tensorflow.keras import layers, models\n\n# Load the dataset\ndata = df_changed\n\n# Handle missing values (e.g., fill NaNs with a method or drop rows)\ndata.fillna(method='ffill', inplace=True)\n\n# Encode 'Class' to binary values (0 for Normal, 1 for Attack)\ndata['Class'] = data['Class'].apply(lambda x: 0 if x == 'Normal' else 1)\n\n# Encode 'Subclass' (if needed)\ndata['SubClass'] = data['SubClass'].fillna('None')\nlabel_encoder = LabelEncoder()\ndata['SubClass'] = label_encoder.fit_transform(data['SubClass'])\n\n# Define features and target variable\nfeatures = ['Arbitration_ID', 'DLC', 'Data[0]', 'Data[1]', 'Data[2]', 'Data[3]', 'Data[4]', 'Data[5]', 'Data[6]', 'Data[7]', 'SubClass']\nX = data[features]\ny = data['Class']\n\n# Normalize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Build the model\nmodel = models.Sequential()\nmodel.add(layers.Dense(64, activation='relu', input_shape=(X_train.shape[1],)))\nmodel.add(layers.Dense(32, activation='relu'))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))  # Output layer for binary classification\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.2)\n\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {test_acc}\")\n\n# Save the model if needed\nmodel.save('model.h5')\n","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:22:46.758198Z","iopub.execute_input":"2024-08-24T13:22:46.75905Z","iopub.status.idle":"2024-08-24T13:23:27.279851Z","shell.execute_reply.started":"2024-08-24T13:22:46.759014Z","shell.execute_reply":"2024-08-24T13:23:27.278159Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom tensorflow.keras import layers, models, callbacks\nfrom tensorflow.keras.utils import to_categorical\n\n# Load and preprocess data\ndata = df_changed \n\n# Handle missing values\ndata.fillna(method='ffill', inplace=True)\n\n# Encode 'Class' to binary values (0 for Normal, 1 for Attack)\ndata['Class'] = data['Class'].apply(lambda x: 0 if x == 'Normal' else 1)\n\n# Encode 'SubClass'\ndata['SubClass'] = data['SubClass'].fillna('None')\nlabel_encoder = LabelEncoder()\ndata['SubClass'] = label_encoder.fit_transform(data['SubClass'])\n\n# Define features and target variable\nfeatures = ['Arbitration_ID', 'DLC', 'Data[0]', 'Data[1]', 'Data[2]', 'Data[3]', 'Data[4]', 'Data[5]', 'Data[6]', 'Data[7]', 'SubClass']\nX = data[features]\ny = data['Class']\n\n# Normalize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Reshape data for CNN (e.g., to 3x3x1 if we want to fit it into a grid)\nX = X.reshape(X.shape[0], 11, 1, 1)\n\n# Convert labels to categorical (if necessary)\ny = to_categorical(y, num_classes=2)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n\n# Build the CNN model\nmodel = models.Sequential()\n\n# First Convolutional Block\nmodel.add(layers.Conv2D(64, (3, 3), strides=(1, 1), input_shape=(11, 1, 1), padding='same', activation='relu'))\nmodel.add(layers.Conv2D(64, (3, 3), strides=(1, 1), padding='same', activation='relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(1, 1)))\n\n# Second Convolutional Block\nmodel.add(layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))\nmodel.add(layers.Conv2D(128, (3, 3), strides=(1, 1), padding='same', activation='relu'))\nmodel.add(layers.MaxPooling2D(pool_size=(1, 1)))\n\n# Third Convolutional Block\nmodel.add(layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))\nmodel.add(layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))\nmodel.add(layers.Conv2D(256, (3, 3), strides=(1, 1), padding='same', activation='relu'))\nmodel.add(layers.GlobalAveragePooling2D())\n\n# Fully Connected Layers\nmodel.add(layers.Dense(256, activation='relu'))\nmodel.add(layers.Dropout(0.5))\nmodel.add(layers.Dense(2, activation='softmax'))  # Output layer for binary classification\n\n# Compile the model\nmodel.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# Callbacks\nearly_stopping = callbacks.EarlyStopping(monitor='val_accuracy', patience=2, verbose=1, mode='auto')\nsave_best_model = callbacks.ModelCheckpoint(filepath='model.weights.h5', monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n\n# Train the model\nhistory = model.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, callbacks=[early_stopping, save_best_model])\n\n# Evaluate the model\ntest_loss, test_acc = model.evaluate(X_test, y_test)\nprint(f\"Test Accuracy: {test_acc}\")\n\n# Plot training history\nimport matplotlib.pyplot as plt\n\nplt.plot(history.history['accuracy'], label='accuracy')\nplt.plot(history.history['val_accuracy'], label = 'val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.show()\n\n","metadata":{"execution":{"iopub.status.busy":"2024-08-24T08:15:28.237446Z","iopub.status.idle":"2024-08-24T08:15:28.237906Z","shell.execute_reply.started":"2024-08-24T08:15:28.237689Z","shell.execute_reply":"2024-08-24T08:15:28.23771Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## VGG16 Model Training","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport tensorflow\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom tensorflow.keras import layers, models, callbacks\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.optimizers import Adam\n\n# Load and preprocess data\ndata = df_changed \n\n# Handle missing values\ndata.fillna(method='ffill', inplace=True)\n\n# Encode 'Class' to binary values (0 for Normal, 1 for Attack)\ndata['Class'] = data['Class'].apply(lambda x: 0 if x == 'Normal' else 1)\n\n# Encode 'SubClass'\ndata['SubClass'] = data['SubClass'].fillna('None')\nlabel_encoder = LabelEncoder()\ndata['SubClass'] = label_encoder.fit_transform(data['SubClass'])\n\n# Define features and target variable\nfeatures = ['Arbitration_ID', 'DLC', 'Data[0]', 'Data[1]', 'Data[2]', 'Data[3]', 'Data[4]', 'Data[5]', 'Data[6]', 'Data[7]', 'SubClass']\nX = data[features]\ny = data['Class']\n\n# Normalize the features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Reshape data for CNN (e.g., to 3x3x1 if we want to fit it into a grid)\nX = X.reshape(X.shape[0], 11, 1, 1)\n\n# Convert labels to categorical (if necessary)\ny = to_categorical(y, num_classes=2)\n\n# Split the data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)# Build the VGG16 model compatible with your data\n\nmodel_vgg16 = models.Sequential()\n\n# Block 1\nmodel_vgg16.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu', input_shape=(11, 1, 1)))\nmodel_vgg16.add(layers.Conv2D(64, (3, 3), padding='same', activation='relu'))\nmodel_vgg16.add(layers.MaxPooling2D(pool_size=(1, 1)))\nmodel_vgg16.add(layers.BatchNormalization())\n\n# Block 2\nmodel_vgg16.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\nmodel_vgg16.add(layers.Conv2D(128, (3, 3), padding='same', activation='relu'))\nmodel_vgg16.add(layers.MaxPooling2D(pool_size=(1, 1)))\nmodel_vgg16.add(layers.BatchNormalization())\n\n# Block 3\nmodel_vgg16.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\nmodel_vgg16.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\nmodel_vgg16.add(layers.Conv2D(256, (3, 3), padding='same', activation='relu'))\nmodel_vgg16.add(layers.MaxPooling2D(pool_size=(1, 1)))\nmodel_vgg16.add(layers.BatchNormalization())\n\n# Block 4\nmodel_vgg16.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\nmodel_vgg16.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\nmodel_vgg16.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\nmodel_vgg16.add(layers.MaxPooling2D(pool_size=(1, 1)))\nmodel_vgg16.add(layers.BatchNormalization())\n\n# Block 5\nmodel_vgg16.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\nmodel_vgg16.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\nmodel_vgg16.add(layers.Conv2D(512, (3, 3), padding='same', activation='relu'))\nmodel_vgg16.add(layers.MaxPooling2D(pool_size=(1, 1)))\nmodel_vgg16.add(layers.BatchNormalization())\n\n# Flatten and Fully Connected Layers\nmodel_vgg16.add(layers.Flatten())\nmodel_vgg16.add(layers.Dense(4096, activation='relu'))\nmodel_vgg16.add(layers.Dropout(0.5))\nmodel_vgg16.add(layers.Dense(4096, activation='relu'))\nmodel_vgg16.add(layers.Dropout(0.5))\nmodel_vgg16.add(layers.Dense(2, activation='softmax'))  # Output layer for binary classification\n\noptimizer = Adam(learning_rate=0.0001)\n# Compile the model\nmodel_vgg16.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n\nearly_stopping = callbacks.EarlyStopping(monitor='val_accuracy', patience=2, verbose=1, mode='auto')\nsave_best_model = callbacks.ModelCheckpoint(filepath='vgg_16model.weights.h5', monitor='val_accuracy', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n\n# Train the model\nhistory_vgg16 = model_vgg16.fit(X_train, y_train, epochs=20, batch_size=32, validation_split=0.2, callbacks=[early_stopping, save_best_model])\n\n# Evaluate the model\ntest_loss_vgg16, test_acc_vgg16 = model_vgg16.evaluate(X_test, y_test)\nprint(f\"Test Accuracy VGG16: {test_acc_vgg16}\")\n\n# Plot training history\nplt.plot(history_vgg16.history['accuracy'], label='accuracy')\nplt.plot(history_vgg16.history['val_accuracy'], label='val_accuracy')\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim([0, 1])\nplt.legend(loc='lower right')\nplt.show()\n","metadata":{"execution":{"iopub.status.busy":"2024-08-24T13:45:31.585668Z","iopub.execute_input":"2024-08-24T13:45:31.586073Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\n\n# Function to compute entropy\ndef get_H(series_aid):\n    count = series_aid.value_counts()\n    p_i = count / series_aid.shape[0]\n    return -(p_i * np.log(p_i)).sum()\n\n# Dataset files\nfiles = ['Pre_train_D_0.csv', 'Pre_train_S_2.csv', 'Pre_train_D_2.csv', \n         'Pre_train_S_0.csv', 'Pre_train_S_1.csv', 'Pre_train_D_1.csv']\ndataset_path = \"/kaggle/input/car-hacking-challenge-dataset/0_Preliminary/0_Training\"\n\n# **Step 1: Gather all unique SubClass values**\nsubclass_values = set()  # Using a set to avoid duplicates\ndf_list = []\n\n# Load dataset function\nfor file in files:\n    df = pd.read_csv(f\"{dataset_path}/{file}\")\n\n    print(f\"Processing: {file} | Columns: {df.columns.tolist()}\")\n\n    # If 'SubClass' exists, collect its unique values\n    if 'SubClass' in df.columns:\n        subclass_values.update(df['SubClass'].dropna().unique())\n    else:\n        df['SubClass'] = \"Unknown\"  # Assign 'Unknown' where SubClass is missing\n\n    df_list.append(df)\n\n# **Step 2: Include 'Unknown' in LabelEncoder training**\nsubclass_values.add(\"Unknown\")  # Ensure 'Unknown' is part of the encoding\nlabel_encoder = LabelEncoder()\nlabel_encoder.fit(list(subclass_values))  # Convert set to list and fit\n\n# **Step 3: Process each dataset and apply the same label encoding**\nprocessed_dfs = []\nfor df in df_list:\n    # Fill missing 'SubClass' with 'Unknown' before encoding\n    df['SubClass'] = df['SubClass'].fillna(\"Unknown\")\n\n    # Convert timestamp features\n    df['abstime'] = pd.to_datetime(df['Timestamp'], unit='s').round('us')\n    df['monotime'] = df['Timestamp'] - df['Timestamp'].min()\n\n    # Convert Arbitration_ID from hex to int\n    df['aid_int'] = df['Arbitration_ID'].map(lambda x: int(x, 16))\n\n    # Encode Class (Normal=0, Attack=1)\n    df['y'] = df['Class'].map({'Normal': 0, 'Attack': 1})\n\n    # Compute time interval per Arbitration_ID\n    df['time_interval'] = df.groupby('Arbitration_ID')['Timestamp'].diff()\n\n    # Convert Data field from hex to numerical\n    df['Data'] = df['Data'].fillna('')  # Fill missing Data with empty string\n    df_data = [[int(i, 16) for i in row.split()]+[-1]*(8-len(row.split())) for row in df['Data']]\n    df_data = pd.DataFrame(df_data, columns=[f'datafield{i}' for i in range(8)])\n\n    # Apply the pre-trained LabelEncoder to 'SubClass'\n    df['SubClass'] = label_encoder.transform(df['SubClass'])\n\n    # Merge processed data\n    df_final = pd.concat([df.reset_index(drop=True), df_data.reset_index(drop=True)], axis=1)\n    processed_dfs.append(df_final)\n\n# Combine datasets\ndf_combined = pd.concat(processed_dfs, ignore_index=True)\n\n# Compute entropy function\ndf_combined['entropy'] = df_combined.rolling(window=2402, min_periods=2402, step=10)['aid_int'].apply(get_H)\ndf_combined['entropy'] = df_combined['entropy'].ffill()\n\n# Remove rows with NaN time_interval or entropy\ndf_combined = df_combined.dropna(subset=['time_interval', 'entropy'])\n\n# Select features and target\nfeatures = ['aid_int', 'time_interval', 'entropy', 'datafield0', 'datafield1', \n            'datafield2', 'datafield3', 'datafield4', 'datafield5', 'datafield6', \n            'datafield7', 'SubClass']\nX = df_combined[features]\ny = df_combined['y']\n\n# Normalize features\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\n\n# Reshape X for CNN or LSTM models\nX = X.reshape(X.shape[0], X.shape[1], 1)\n\n# Save processed data\ndf_combined.to_csv(\"processed_dataset.csv\", index=False)\n\nprint(f\"✅ Preprocessing complete! Processed data shape: {X.shape}\")\n\n# Print the mapping of SubClass labels\nmapping = {index: label for index, label in enumerate(label_encoder.classes_)}\nprint(\"📌 SubClass Mapping:\", mapping)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-25T05:16:04.814053Z","iopub.execute_input":"2025-03-25T05:16:04.814717Z","iopub.status.idle":"2025-03-25T05:20:52.695369Z","shell.execute_reply.started":"2025-03-25T05:16:04.814690Z","shell.execute_reply":"2025-03-25T05:20:52.694415Z"}},"outputs":[{"name":"stdout","text":"Processing: Pre_train_D_0.csv | Columns: ['Timestamp', 'Arbitration_ID', 'DLC', 'Data', 'Class']\nProcessing: Pre_train_S_2.csv | Columns: ['Timestamp', 'Arbitration_ID', 'DLC', 'Data', 'Class', 'SubClass']\nProcessing: Pre_train_D_2.csv | Columns: ['Timestamp', 'Arbitration_ID', 'DLC', 'Data', 'Class', 'SubClass']\nProcessing: Pre_train_S_0.csv | Columns: ['Timestamp', 'Arbitration_ID', 'DLC', 'Data', 'Class']\nProcessing: Pre_train_S_1.csv | Columns: ['Timestamp', 'Arbitration_ID', 'DLC', 'Data', 'Class', 'SubClass']\nProcessing: Pre_train_D_1.csv | Columns: ['Timestamp', 'Arbitration_ID', 'DLC', 'Data', 'Class', 'SubClass']\n✅ Preprocessing complete! Processed data shape: (3669343, 12, 1)\n📌 SubClass Mapping: {0: 'Flooding', 1: 'Fuzzing', 2: 'Normal', 3: 'Replay', 4: 'Spoofing', 5: 'Unknown'}\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"import pandas as pd\n\n# Path to the Kaggle output directory\ninput_path = \"/kaggle/working/large_file.xlsx\"  # Adjust if needed\noutput_path = \"/kaggle/working/output.csv\"\n\n# Load the Excel file\ndf = pd.read_excel(input_path, engine=\"openpyxl\")\n\n# Save as CSV\ndf.to_csv(output_path, index=False)\n\nprint(\"Conversion complete! CSV saved at:\", output_path)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}